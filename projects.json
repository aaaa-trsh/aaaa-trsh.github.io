{
  "Wave Function Collapse Labs": {
    "year": 2020,
    "tag": "other",
    "link": "",
    "image": "images/projects/wfc_labs.png",
    "title": "",
    "description": "Generating a cell based enviornment with Wave Function Collapse and Marching Cubes."
  },
  "Q-Learning Snakes": {
    "year": 2020,
    "tag": "ml",
    "link": "https://noof125.itch.io/rl-snakes",
    "image": "images/projects/rl_snakes.PNG",
    "title": "Auto Snakes on itch.io",
    "description": "Using Q-Learning play snake or act as nuisances."
  },
  "MONS": {
    "year": 2020,
    "tag": "game",
    "link": "https://jovlett.itch.io/mons",
    "image": "images/projects/mons.png",
    "title": "MONS on itch.io",
    "description": "A short walk on a 2D mountain with the help of 3 dimensions. [Brackeys Jam 2020.2]"
  },
  "FRC Advanced Drive + Odometry": {
    "year": 2020,
    "tag": "robotics",
    "link": "",
    "image": "images/projects/advanced_frc2020.PNG",
    "title": "FRC2021 GitHub",
    "description": "Robot code with odometry and trajectory planning, based on off-season drivebase."
  },
  "FRC Mockup Robot Design and Code": {
    "year": 2020,
    "tag": "robotics",
    "link": "https://github.com/aaaa-trsh/6644-Offseason-Code",
    "image": "images/projects/code_frc2021.PNG",
    "title": "6644's 2020 Offseason Robot Code",
    "description": "Offseason robot design for 2020, with accomodating prototype code."
  },
  "Infinite Recharge Robot Design and Code": {
    "year": 2020,
    "tag": "robotics",
    "link": "https://github.com/aaaa-trsh/_FRC2020",
    "image": "images/projects/code_frc2020.PNG",
    "title": "6644's 2020 Robot Code",
    "description": "Robot design for 2020, with the code we used during the season."
  },
  "FRC Path Planning and Generation": {
    "year": 2020,
    "tag": "robotics",
    "link": "http://www.joathrent.com/frc-path-planner/index.html",
    "image": "images/projects/path_planner2020.jpg",
    "title": "FRC Path Planner",
    "description": "A path planning app in JavaScript that generates commands for autonomous drive."
  },
  "Generating Bach and Mozart with RNNs": {
    "year": 2020,
    "tag": "ml",
    "link": "",
    "image": "images/projects/music_rnn.jpg",
    "title": "",
    "description": "An RNN that generates music in the style baroque/classical music.",
    "event": "setcurrentproject(0)"
  },
  "VAE Yearbook": {
    "year": 2020,
    "tag": "ml",
    "link": "./article?id=8",
    "image": "images/projects/faceapp.jpg",
    "title": "VAE Article and Face Tool",
    "description": "An autoencoder that can blend facial features, based on people from my school.",
    "tool": "<faceapp id='faceapp'></faceapp>",
    "article": {
      "0": "Yearbooks are a window to the past. The end of a school year is a time like no other, a time when old friends are together, when everything is simple, and when that one math teacher says that bell doesn't excuse you, I do. \n\nIt also gives a TON of training data! \n\nIn this project, I use around 3,000 images of my fellow students to make a face blending experiment.",
      "What even is this thing?": "Well, its a Convolutional Variational Autoencoder. \nAn autoencoder is a way of recreating some data by compressing it into a couple of values, and reconstructing it based off of that encoded data. This uses 2 neural networks:\n\n - The Encoder which encodes the input into a super small amount of data. In this case, I used 20 values to represent a 64 x 64 image.\n - The Decoder which uses the <i>dense</i>, highly compressed version of the image to try to recreate the image as closely as possible. \n\nIt also uses convolutional layers to extract defining information from images to encode and decode. And, its a variational autoencoder, which means that it tries to center points around the origin of the latent space. This is so that when moving between two faces, you won't get any garbage results that the autoencoder hasn't seen before. I made this one in Tensorflow.",
      "Collecting Data": "I used this year's high school yearbook as training data, as the photos are very consistantly lit and all have a similar background. At first I tried to take pictures with my phone, but the uneven lighting in my room, the shaking camera, and inconsistent folding of pages created a lot of glare, blur, and generally unusable samples. Instead, I used a scanner to scan the pages, which made clearly defined faces much more consistant at the cost of adding fine noise and leaving some blurry samples near the middle of the book.",
      "Processing Data": "To extract all of these faces, I used the OpenCV haarcascade. I made a helper script that improved sample collection, so that instead of blankly searching for images, it used detected faces as landmarks and generated a grid of expected face locations from them. From there, all positions in this grid were cropped and saved. It was relatively easy to pick out spots that did not include a face (took around an hour), and I didn't do this in the code because some samples would be lost. \n\nThe reason I made this was to increase the amount of samples that OpenCV would detect. Just running the haarcascade over the entire image would leave out people based on their make-up, expression, and hairstyle. \n\nOverall I collected a decent 2,012 faces from 56 pages.",
      "Training": "After collecting the data and building the model, I started training. Everything was smooth sailing until about 48 hours in, wherein I noticed that I was using the same data for training and testing. This meant that the autoencoder was essentially memorizing sequences of IDs instead of interpreting any useful generalizations. After another gruesome 2 days, here are some results: <div class='article__gallery'><img src='./images/facevae/faceGrid1.png' /><img src='./images/facevae/faceGrid2.png' /><img src='./images/facevae/faceGrid3.png' /><p class='caption'>Hey, I can recognize some of these people!</p><div/>",
      "Making It An App": "After repeatedly having to use the command prompt to summon my program for training and generation, I finally had enough. So, I learned tkinter, a popular GUI tool for python, and made a neat little UI for the app. <div class='article__gallery'><img src='./images/facevae/pyapp.png' /><p class='caption'>GUI Tool</p><div/>",
      "0000": "At this point, I thought I was pretty much done with this project, and was satisfied with the results.",
      "Making It An App Part 2": "...or so I thought. After asking feedback from people smarter than me, I came to the realization that I should put this in a website. So, I made a janky version of what is now my portfolio, and then made a bunch of UI for it in HTML and CSS. Unfortunately, for around a week, I was putting off actually putting the model into the website. After trying and failing to port my custom model into Tensorflow.js, I realized that I had to split it up, as the model was basically 2 separate neural networks anyways. After making all of the UI tools work correctly, I had a pretty viable tool. \n\nIn this version of the tool, I also added interpolation, something I've seen in many other similar implementations. <div class='article__gallery'><img src='./images/facevae/webapp.PNG' /><p class='caption'>The old face tool. It's ugly but it works.</p><div/>",
      "00": "By interpolating between all faces, this is the average person at my high school: <div class='article__gallery'><img src='./images/facevae/avgJoe.png' /><p class='caption'>Its the average Joe.</p><div/>",
      "What I learned": "This project went a lot smoother than the last one. I learned how to collect data consistently and accurately, which is a big step up from just blindly downloading data from the internet like I did in the last one. I learned about making UI in tkinter, more about Tensorflow, and deployed my first interactive non-game project. Iteration was still a big issue, but now, with around 2000 faces, it was much worse. I would usually have to run my computer for around a day and a half just to see any results, and I remember doing this around 3 times before ironing out all the bugs. Now that I have a better computer, I am interested to see how it will do with this project and my previous eye generation project. I might also do another data collection expirement to be able to get named pictures from the entire yearbook."
    }
  },
  "Eye Generation with GANs": {
    "year": 2020,
    "tag": "ml",
    "link": "./article?id=9",
    "image": "images/projects/eyegan.jpg",
    "title": "GAN Article",
    "description": "A GAN that can generate pictures of eyes.",
    "article": {
      "0": "As a beginner in the field of Neural Networks, I believe that image generation is the best invention since sliced bread. From faces to handwritten numbers, the amount of growth in this field is pretty amazing. This is a project I did to get myself somewhat familiar with it.",
      "What even is this thing?": "Well, its a GAN. \nA GAN (or General Adversarial Network) is a way of generating an output based off of 2 neural networks:\n\n - The Generator which spits out random noise and slowly learns to replicate a structure based on feedback.\n - The Discriminator which is given a mix of real and fake inputs and tries to pick out which is real. \n\nMore specifically, its a DCGAN, which uses convolutional layers to find and filter specific features in images, making it ideal for creating them.\n\nDuring training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. As the GAN converges, the 2 neural networks (should) reach an equilibrium when the discriminator can no longer distinguish real images from fake, generated images.",
      "Collecting Data": "To collect the data for this project, I used a modified a web scraper. It works by entering the search query into Bing Image Search, grabbing all of the <img> tags, and downloading their source. Originally, I just downloaded them manually, but this strategy was wholly ineffective once I needed more data. <div class='article__gallery'><img src='./images/eyegan/data.gif' /><p class='caption'>Web scraper</p><div/>",
      "Processing Data": "I originally made the GAN create handwritten images based off of the MNIST dataset, which is a huge collection of handwritten digits from 0 to 9. I was initially confused about how these digits were stored, as there were no explicit image files, but after rummaging though the MNIST dataset for a while, I noticed it was just a big 2 dimentional array of brightness values. So, to replicate the MNIST data, I resized all of my data, converted it to grayscale, and stored that into a numpy array. With some minor tweaks, the neural network now produced 64 by 64 images: <div class='article__gallery'><img src='./images/eyegan/blobG1.gif' /><img src='./images/eyegan/blobG2.gif' /><img src='./images/eyegan/blobG3.gif' /><img src='./images/eyegan/blobG4.gif' /><p class='caption'>The first couple attempts at generating eyes</p><div/>",
      "Tuning and adjustments": "After consulting with people way smarter than I am, we agreed that there were 2 big problems with my GAN: the amount of data I used is tiny, and it only ran for 50 generations. In the failed iteration, I only used 87 hand downloaded images, and this was nowhere near large enough. So, I increased the training time to 250 epochs (training generations) and tripled the dataset to 300 images, and got some somewhat better results. <div class='article__gallery'><img src='./images/eyegan/goodG1.gif' /><img src='./images/eyegan/goodG2.gif' /><img src='./images/eyegan/goodG3.gif' /><p class='caption'>Second generation attempt: much better!</p><div/>",
      "00": "Looking back at it, 300 images is nowhere near the amount of images I should've had, but those extra 200 epochs really helped.",
      "Lets add some color!": "This is cool and all, but I'm getting board of looking at a bunch of gray squares.\n\nColor is usually represented in RGB for these projects, which is made of 3 brightness values for each color channel in a pixel. Converting the data into RGB was easy, because all I had to do was to tell the data loader to not grayscale it. \n\nTo force the neural networks to recognize this color, I swapped their inputs from (n, 64, 64, 1) to (n, 64, 64, 3). 'n' being the number of images in the dataset, 64 being the width and height of the input, and 3 being the color channels. Heres what that looked like: <div class='article__gallery'><img src='./images/eyegan/boostedRGB1.gif' /><img src='./images/eyegan/boostedRGB2.gif' /><img src='./images/eyegan/boostedRGB3.gif' /><p class='caption'>Weird, boosted RGB generations</p><div/>",

      "000": "I spent 2 entire days messing around in the code and noticed that multiplying the colors by 255 boosted the saturation and brightness way too much. Apparently, the values I was getting from the generator was from -1 to 1 instead of 0 to 1, and by multiplying them, I got a color range that was -255 to 255. What this did was it turned midtone and darker colors black, and made lighter colors fully saturated. ",
      "00000": "<pre><code class='language-python'># Paste generated images into a grid and save\ndef save_output(predictions, scale):\n&emsp;result = new Image('RGB', (64 * scale, 64 * scale))\n&emsp;i = 0\n&emsp;for y in range(0, scale):\n&emsp;&emsp;for x in range(0, scale):\n&emsp;&emsp;&emsp;img_np = (predictions[i].numpy());\n&emsp;&emsp;&emsp;img = Image.fromarray((img_np * 255).astype(np.uint8)) # This is the problem area\n&emsp;&emsp;&emsp;result.paste(img, (y * 64, x * 64))\n&emsp;&emsp;&emsp;i += 1\n&emsp;&emsp;&emsp;result.save('generation{:04d}.png'.format(epoch))</code></pre>",
      "000000": "With some basic math I sorted it out, but diagnosing the issue was a headache. The fixed up version looks like this:<div class='article__gallery'><img src='./images/eyegan/goodRGB1.gif' /><img src='./images/eyegan/goodRGB2.gif' /><img src='./images/eyegan/goodRGB3.gif' /><p class='caption'>So much better!</p><div/>",
      "0000": "And thats it! Sure, it definitely could use a little bit more training time, but I'd say that those are pretty good for now. I'm going to revisit this project in the future, hopefully with a better computer so that iteration can be faster. <div class='article__gallery'><img src='./images/eyegan/goodRGB4.png' /><img src='./images/eyegan/goodRGB5.png' /><img src='./images/eyegan/goodRGB6.png' /><p class='caption'>More results</p><div/>",
      "00000000": "In more fully fleshed out images, you can really see the 'eye' part of it. For every picture, it includes the general structure of the eye: the iris and pupil are always somewhere, and most of them have an eliptical shape.",
      "What I learned": "Machine learning projects take a lot of iteration, and a lot of unexpected problems can occur. Because everything is relating to one type of data - in my case, numpy arrays - there was a multitude of different operations that could've led to a poor result. Another big issue I ran into was the amount of time each iteration took. In order to test my model, I would need to leave my computer training for days to even get a peek at any issues, which led to this project taking multiple months. I should probably look into cloud computing to be able to fix, iterate, and better it more quickly.",
      "Conclusion": "A lot of interesting stuff can be generated with GANs. This small project is nowhere near the state-of-the-art tech that researchers are working on, but even so, it is interesting to see what a basic set of neural networks can do. More complex models like NVIDIA's StyleGAN2, BigGan, and StyleALAE can generate very high fidelity images that are almost indistinguishable from reality, which goes to show the massive growth this section of machine learning is receiving."
    }
  },
  "Q-Learning Tic Tac Toe": {
    "year": 2020,
    "tag": "ml",
    "link": "",
    "image": "images/projects/tictactoe.jpg",
    "title": "",
    "description": "Using Q Learning to play tic tac toe."
  },
  "WORMHOLES n' Chill": {
    "year": 2020,
    "tag": "game",
    "link": "https://noof125.itch.io/wormholes-n-chill",
    "image": "images/projects/wormholesnchill.PNG",
    "title": "Wormholes n' Chill on itch.io",
    "description": "Ever want to chill n' drop some wormholes? [Brackeys Jam 2020.1]"
  },
  "Dust and Diamonds": {
    "year": 2019,
    "tag": "game",
    "link": "https://play.google.com/store/apps/details?id=com.JoathrentStudios.DustAndDiamonds",
    "image": "images/projects/dustanddiamonds.jpg",
    "title": "Dust and Diamonds on Google Play",
    "description": "A match 3 idle game about mining rocks and matching gems!"
  },
  "groupThink": {
    "year": 2019,
    "tag": "game",
    "link": "https://noof125.itch.io/groupthink",
    "image": "images/projects/groupthink.PNG",
    "title": "groupThink on itch.io",
    "description": "Escape a dangerous mob with the power of true sight. [Global Game Jam Next 2019]"
  },
  "How Our Mission Ended": {
    "year": 2019,
    "tag": "game",
    "link": "https://globalgamejam.org/2019/games/how-our-mission-ended",
    "image": "images/projects/howourmissionended.PNG",
    "title": "How Our Mission Ended on GlobalGameJam.org",
    "description": "A tricky game about interplanetary interactions [Global Game Jam 2019]"
  },
  "Grid Gunner": {
    "year": 2018,
    "tag": "game",
    "link": "https://play.google.com/store/apps/details?id=com.JoathrentStudios.GridGunner",
    "image": "images/projects/gridgunner.PNG",
    "title": "Grid Gunner on Google Play",
    "description": "A frantic, turn-based bullet hell where you infiltrate an enemy compound."
  }
}